{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO4kzD0dGrJRJaWeb96fZK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danendra10/Learning-Neural-Networks/blob/main/With_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uBikOn7jEEKb"
      },
      "outputs": [],
      "source": [
        "# import tensorflow library\n",
        "# Since we'll be using functionalities\n",
        "# of tensorflow V1 Let us import Tensorflow v1\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import numpy as np\n",
        "\n",
        "# Create placeholders for input X and output Y\n",
        "X = tf.placeholder(dtype=tf.float32, shape=(16, 4))\n",
        "Y = tf.placeholder(dtype=tf.float32, shape=(16, 1))\n",
        "\n",
        "# Give training input and label\n",
        "INPUT_XOR = [\n",
        "      [0,0,0,0],\n",
        "      [0,0,0,1],\n",
        "      [0,0,1,0],\n",
        "      [0,0,1,1],\n",
        "      [0,1,0,0],\n",
        "      [0,1,0,1],\n",
        "      [0,1,1,1],\n",
        "      [1,0,0,0],\n",
        "      [1,0,0,1],\n",
        "      [1,0,1,0],\n",
        "      [1,0,1,1],\n",
        "      [1,1,0,0],\n",
        "      [1,1,0,1],\n",
        "      [1,1,0,0],\n",
        "      [1,1,1,0],\n",
        "      [1,1,1,1]\n",
        "]\n",
        "OUTPUT_XOR = [\n",
        "    [0],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [1],\n",
        "    [0]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Give a standard learning rate and the number\n",
        "# of epochs the model has to train for.\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "# Create/Initialize a Hidden Layer variable\n",
        "with tf.variable_scope('hidden'):\n",
        "\n",
        "\t# Initialize weights and biases for the\n",
        "\t# hidden layer randomly whose mean=0 and\n",
        "\t# std_dev=1\n",
        "\th_w = tf.Variable(tf.truncated_normal([4, 2]), name='weights')\n",
        "\th_b = tf.Variable(tf.truncated_normal([16, 1]), name='biases')\n",
        "\t\n",
        "\t# Pass the matrix multiplied Input and\n",
        "\t# weights added with Bias to the relu\n",
        "\t# activation function\n",
        "\th = tf.nn.relu(tf.matmul(X, h_w) + h_b)\n",
        "\n",
        "# Create/Initialize an Output Layer variable\n",
        "with tf.variable_scope('output'):\n",
        "\t\n",
        "\t# Initialize weights and biases for the\n",
        "\t# output layer randomly whose mean=0 and\n",
        "\t# std_dev=1\n",
        "\to_w = tf.Variable(tf.truncated_normal([2, 1]), name='weights')\n",
        "\to_b = tf.Variable(tf.truncated_normal([16, 1]), name='biases')\n",
        "\t\n",
        "\t# Pass the matrix multiplied hidden layer\n",
        "\t# Input and weights added with Bias\n",
        "\t# to a sigmoid activation function\n",
        "\tY_estimation = tf.nn.sigmoid(tf.matmul(h, o_w) + o_b)\n",
        "\n",
        "# Create/Initialize Loss function variable\n",
        "with tf.variable_scope('cost'):\n",
        "\n",
        "\t# Calculate cost by taking the Root Mean\n",
        "\t# Square between the estimated Y value\n",
        "\t# and the actual Y value\n",
        "\tcost = tf.reduce_mean(tf.squared_difference(Y_estimation, Y))\n",
        "\n",
        "# Create/Initialize Training model variable\n",
        "with tf.variable_scope('train'):\n",
        "\n",
        "\t# Train the model with ADAM Optimizer\n",
        "\t# with the previously initialized learning\n",
        "\t# rate and the cost from the previous variable\n",
        "\ttrain = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# Start a Tensorflow Session\n",
        "with tf.Session() as session:\n",
        "\n",
        "\t# initialize the session variables\n",
        "\tsession.run(tf.global_variables_initializer())\n",
        "\tprint(\"Training Started\")\n",
        "\t\n",
        "\t# log count\n",
        "\tlog_count_frac = epochs/10\n",
        "\tfor epoch in range(epochs):\n",
        "\t\n",
        "\t\t# Training the base network\n",
        "\t\tsession.run(train, feed_dict={X: INPUT_XOR, Y:OUTPUT_XOR})\n",
        "\n",
        "\t\t# log training parameters\n",
        "\t\t# Print cost for every 1000 epochs\n",
        "\t\tif epoch % log_count_frac == 0:\n",
        "\t\t\tcost_results = session.run(cost, feed_dict={X: INPUT_XOR, Y:OUTPUT_XOR})\n",
        "\t\t\tprint(\"Cost of Training at epoch {0} is {1}\".format(epoch, cost_results))\n",
        "\n",
        "\tprint(\"Training Completed !\")\n",
        "\tY_test = session.run(Y_estimation, feed_dict={X:INPUT_XOR})\n",
        "\tprint(np.round(Y_test, decimals=1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeGbdXXjEhKQ",
        "outputId": "ec40c02c-2a8d-4a49-c11a-38a74386e089"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Started\n",
            "Cost of Training at epoch 0 is 0.20121651887893677\n",
            "Cost of Training at epoch 1000 is 0.0009115401189774275\n",
            "Cost of Training at epoch 2000 is 0.00024386291624978185\n",
            "Cost of Training at epoch 3000 is 0.00010356835264246911\n",
            "Cost of Training at epoch 4000 is 5.205215347814374e-05\n",
            "Cost of Training at epoch 5000 is 2.4787681468296796e-05\n",
            "Cost of Training at epoch 6000 is 1.409145443176385e-05\n",
            "Cost of Training at epoch 7000 is 8.212702596210875e-06\n",
            "Cost of Training at epoch 8000 is 4.857673957303632e-06\n",
            "Cost of Training at epoch 9000 is 2.89943045572727e-06\n",
            "Training Completed !\n",
            "[[0.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [1.]\n",
            " [0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try With Azzam's Datas"
      ],
      "metadata": {
        "id": "vxez-T59G2Z8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrNfA04UEm6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}